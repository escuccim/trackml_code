{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from trackml.dataset import load_event\n",
    "from sklearn import cluster, preprocessing\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trackml.score import score_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1770\n",
      "Test: 125\n",
      "Detectors: 18728\n",
      "Sample Submission: 13741466\n"
     ]
    }
   ],
   "source": [
    "train = np.unique([p.split('-')[0] for p in sorted(glob.glob('./data/train_1/**'))])\n",
    "test = np.unique([p.split('-')[0] for p in sorted(glob.glob('./data/test/**'))])\n",
    "det = pd.read_csv('./data/detectors.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "print(\"Train:\",len(train)) \n",
    "print(\"Test:\", len(test))\n",
    "print(\"Detectors:\", len(det))\n",
    "print(\"Sample Submission:\", len(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def get_training_sample(path_to_data, event_names):\n",
    "\n",
    "    events = []\n",
    "    track_id = 0\n",
    "\n",
    "    for name in tqdm(event_names):\n",
    "        # if there is an error skip it\n",
    "        try:\n",
    "            # Read an event\n",
    "            hits, cells, particles, truth = load_event(os.path.join(path_to_data, name))\n",
    "\n",
    "            # Generate new vector of particle id\n",
    "            particle_ids = truth.particle_id.values\n",
    "            particle2track = {}\n",
    "            for pid in np.unique(particle_ids):\n",
    "                particle2track[pid] = track_id\n",
    "                track_id += 1\n",
    "            hits['particle_id'] = [particle2track[pid] for pid in particle_ids]\n",
    "\n",
    "            # Collect hits\n",
    "            events.append(hits)\n",
    "        except:\n",
    "            print(\"Error with\", name)\n",
    "            continue\n",
    "            \n",
    "    # Put all hits into one sample with unique track ids\n",
    "    data = pd.concat(events, axis=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this according to your directory preferred setting\n",
    "path_to_train = \"data/train_1\"\n",
    "\n",
    "# This event is in Train_1\n",
    "event_prefix = \"event000001000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:03<00:08,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with event000001006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: (2169884, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_event_id = 1000\n",
    "n_train_samples = 10\n",
    "train_event_names = [\"event0000{:05d}\".format(i) for i in range(start_event_id, start_event_id+n_train_samples)]\n",
    "train_data = get_training_sample(path_to_train, train_event_names)\n",
    "print(\"train_data:\", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>volume_id</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>module_id</th>\n",
       "      <th>particle_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-64.409897</td>\n",
       "      <td>-7.163700</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-55.336102</td>\n",
       "      <td>0.635342</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-83.830498</td>\n",
       "      <td>-1.143010</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-96.109100</td>\n",
       "      <td>-8.241030</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-62.673599</td>\n",
       "      <td>-9.371200</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hit_id          x         y       z  volume_id  layer_id  module_id  \\\n",
       "0       1 -64.409897 -7.163700 -1502.5          7         2          1   \n",
       "1       2 -55.336102  0.635342 -1502.5          7         2          1   \n",
       "2       3 -83.830498 -1.143010 -1502.5          7         2          1   \n",
       "3       4 -96.109100 -8.241030 -1502.5          7         2          1   \n",
       "4       5 -62.673599 -9.371200 -1502.5          7         2          1   \n",
       "\n",
       "   particle_id  \n",
       "0            0  \n",
       "1          477  \n",
       "2            0  \n",
       "3         3556  \n",
       "4         4811  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "dbscan = cluster.DBSCAN(eps=0.00515, min_samples=1, algorithm='kd_tree', n_jobs=-1)\n",
    "events = []\n",
    "track_id = 0\n",
    "\n",
    "for e in train[:5]:\n",
    "    hits, cells, particles, truth = load_event(e)\n",
    "    hits['event_id'] = int(e[-9:])\n",
    "    cells = cells.groupby(by=['hit_id'])['ch0', 'ch1', 'value'].agg(['mean']).reset_index()\n",
    "    cells.columns = ['hit_id', 'ch0', 'ch1', 'value']\n",
    "    hits = pd.merge(hits, cells, how='left', on='hit_id')\n",
    "    col = [c for c in hits.columns if c not in ['event_id', 'hit_id']]\n",
    "    \n",
    "    # Generate new vector of particle id\n",
    "    particle_ids = truth.particle_id.values\n",
    "    particle2track = {}\n",
    "    for pid in np.unique(particle_ids):\n",
    "        particle2track[pid] = track_id\n",
    "        track_id += 1\n",
    "    hits['particle_id'] = [particle2track[pid] for pid in particle_ids]\n",
    "    \n",
    "    #https://www.kaggle.com/mikhailhushchyn/dbscan-benchmark\n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x'] = x/r\n",
    "    hits['y'] = y/r\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z'] = z/r\n",
    "    \n",
    "    events.append(hits)\n",
    "\n",
    "events = pd.concat(events, axis=0)\n",
    "y = events.particle_id.values\n",
    "X = events.drop(\"particle_id\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='kd_tree', eps=0.00515, leaf_size=30, metric='euclidean',\n",
       "    metric_params=None, min_samples=1, n_jobs=-1, p=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_event = os.path.join(path_to_train, \"event0000{:05d}\".format(start_event_id + n_train_samples + 1))\n",
    "hits, cells, particles, truth = load_event(path_to_event)\n",
    "\n",
    "# Warning: it takes about 30s per one event\n",
    "labels = dbscan.fit_predict(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['particle_id'] = dbscan.fit_predict(scl.fit_transform(hits[['x2', 'y2', 'z2']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/test/event000000000 77379\n",
      "./data/test/event000000001 79199\n",
      "./data/test/event000000002 70258\n",
      "./data/test/event000000003 71722\n",
      "./data/test/event000000004 77982\n",
      "./data/test/event000000005 70888\n",
      "./data/test/event000000006 71302\n",
      "./data/test/event000000007 76779\n",
      "./data/test/event000000008 75379\n",
      "./data/test/event000000009 74109\n",
      "./data/test/event000000010 76395\n",
      "./data/test/event000000011 70509\n",
      "./data/test/event000000012 68967\n",
      "./data/test/event000000013 80624\n",
      "./data/test/event000000014 68400\n",
      "./data/test/event000000015 61685\n",
      "./data/test/event000000016 64732\n",
      "./data/test/event000000017 83900\n",
      "./data/test/event000000018 65388\n",
      "./data/test/event000000019 69323\n",
      "./data/test/event000000020 55907\n",
      "./data/test/event000000021 70486\n",
      "./data/test/event000000022 78410\n",
      "./data/test/event000000023 76052\n",
      "./data/test/event000000024 76067\n",
      "./data/test/event000000025 72464\n",
      "./data/test/event000000026 64874\n",
      "./data/test/event000000027 66324\n",
      "./data/test/event000000028 71436\n",
      "./data/test/event000000029 60388\n",
      "./data/test/event000000030 73178\n",
      "./data/test/event000000031 68523\n",
      "./data/test/event000000032 57234\n",
      "./data/test/event000000033 58773\n",
      "./data/test/event000000034 81566\n",
      "./data/test/event000000035 69069\n",
      "./data/test/event000000036 73278\n",
      "./data/test/event000000037 79703\n",
      "./data/test/event000000038 70044\n",
      "./data/test/event000000039 78378\n",
      "./data/test/event000000040 77113\n",
      "./data/test/event000000041 72749\n",
      "./data/test/event000000042 72508\n",
      "./data/test/event000000043 72889\n",
      "./data/test/event000000044 86993\n",
      "./data/test/event000000045 66361\n",
      "./data/test/event000000046 64561\n",
      "./data/test/event000000047 60813\n",
      "./data/test/event000000048 69324\n",
      "./data/test/event000000049 77111\n",
      "./data/test/event000000050 77024\n",
      "./data/test/event000000051 73831\n",
      "./data/test/event000000052 74666\n",
      "./data/test/event000000053 70919\n",
      "./data/test/event000000054 77268\n",
      "./data/test/event000000055 65908\n",
      "./data/test/event000000056 74299\n",
      "./data/test/event000000057 79879\n",
      "./data/test/event000000058 77870\n",
      "./data/test/event000000059 73682\n",
      "./data/test/event000000060 68638\n",
      "./data/test/event000000061 69757\n",
      "./data/test/event000000062 73617\n",
      "./data/test/event000000063 76384\n",
      "./data/test/event000000064 75043\n",
      "./data/test/event000000065 64494\n",
      "./data/test/event000000066 72527\n",
      "./data/test/event000000067 80503\n",
      "./data/test/event000000068 71894\n",
      "./data/test/event000000069 67484\n",
      "./data/test/event000000070 67697\n",
      "./data/test/event000000071 76452\n",
      "./data/test/event000000072 70316\n",
      "./data/test/event000000073 72643\n",
      "./data/test/event000000074 74650\n",
      "./data/test/event000000075 66174\n",
      "./data/test/event000000076 68728\n",
      "./data/test/event000000077 61918\n",
      "./data/test/event000000078 72900\n",
      "./data/test/event000000079 62799\n",
      "./data/test/event000000080 82238\n",
      "./data/test/event000000081 70850\n",
      "./data/test/event000000082 70836\n",
      "./data/test/event000000083 81362\n",
      "./data/test/event000000084 65058\n",
      "./data/test/event000000085 74258\n",
      "./data/test/event000000086 79242\n",
      "./data/test/event000000087 77406\n",
      "./data/test/event000000088 76516\n",
      "./data/test/event000000089 77617\n",
      "./data/test/event000000090 73313\n",
      "./data/test/event000000091 68985\n",
      "./data/test/event000000092 66519\n",
      "./data/test/event000000093 84685\n",
      "./data/test/event000000094 65348\n",
      "./data/test/event000000095 75995\n",
      "./data/test/event000000096 69502\n",
      "./data/test/event000000097 73217\n",
      "./data/test/event000000098 75358\n",
      "./data/test/event000000099 75798\n",
      "./data/test/event000000100 82919\n",
      "./data/test/event000000101 81049\n",
      "./data/test/event000000102 74291\n",
      "./data/test/event000000103 67895\n",
      "./data/test/event000000104 76889\n",
      "./data/test/event000000105 62171\n",
      "./data/test/event000000106 74256\n",
      "./data/test/event000000107 82942\n",
      "./data/test/event000000108 74485\n",
      "./data/test/event000000109 75827\n",
      "./data/test/event000000110 74751\n",
      "./data/test/event000000111 76066\n",
      "./data/test/event000000112 71421\n",
      "./data/test/event000000113 81020\n",
      "./data/test/event000000114 65724\n",
      "./data/test/event000000115 67143\n",
      "./data/test/event000000116 76493\n",
      "./data/test/event000000117 75611\n",
      "./data/test/event000000118 70312\n",
      "./data/test/event000000119 60067\n",
      "./data/test/event000000120 70110\n",
      "./data/test/event000000121 75577\n",
      "./data/test/event000000122 62339\n",
      "./data/test/event000000123 68124\n",
      "./data/test/event000000124 71273\n"
     ]
    }
   ],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "dbscan = cluster.DBSCAN(eps=0.00515, min_samples=1, algorithm='kd_tree', n_jobs=-1)\n",
    "df_test = []\n",
    "for e in test:\n",
    "    hits, cells = load_event(e, parts=['hits', 'cells'])\n",
    "    hits['event_id'] = int(e[-9:])\n",
    "    cells = cells.groupby(by=['hit_id'])['ch0', 'ch1', 'value'].agg(['mean']).reset_index()\n",
    "    cells.columns = ['hit_id', 'ch0', 'ch1', 'value']\n",
    "    hits = pd.merge(hits, cells, how='left', on='hit_id')\n",
    "    col = [c for c in hits.columns if c not in ['event_id', 'hit_id', 'particle_id']]\n",
    "\n",
    "    #https://www.kaggle.com/mikhailhushchyn/dbscan-benchmark\n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x2'] = x/r\n",
    "    hits['y2'] = y/r\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z2'] = z/r\n",
    "    hits['particle_id'] = dbscan.fit_predict(scl.fit_transform(hits[['x2', 'y2', 'z2']].values))\n",
    "    \n",
    "    df_test.append(hits[['event_id','hit_id','particle_id']].copy())\n",
    "    print(e, len(hits['particle_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat(df_test, ignore_index=True)\n",
    "\n",
    "sub_new = pd.merge(sub, df_test, how='left', on=['event_id','hit_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_new['track_id'] = sub_new['particle_id'] + 1\n",
    "sub_new[['event_id','hit_id','track_id']].to_csv('20180627_dbscan_submission_04.csv.gz', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

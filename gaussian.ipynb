{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a7e58977b0e04bb0518a7da572956a4af46543e"
   },
   "source": [
    "# About\n",
    "\n",
    "This notebook helps you to create your first solution and the first submisson file. Fill free to modify this notebook to create you own solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "8d932ac35079ce9c7ed586984c261dfe2c09c1d5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0636aa35234d8ca2b2d7286727df5db410b8cec9"
   },
   "outputs": [],
   "source": [
    "# Change this according to your directory preferred setting\n",
    "path_to_train = \"data/train_1\"\n",
    "\n",
    "# This event is in Train_1\n",
    "event_prefix = \"event000001000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "30f6d69832eb6b6963721bc766cba0089765d7e9"
   },
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def get_training_sample(path_to_data, event_names):\n",
    "\n",
    "    events = []\n",
    "    track_id = 0\n",
    "\n",
    "    for name in tqdm(event_names):\n",
    "        # if there is an error skip it\n",
    "        try:\n",
    "            # Read an event\n",
    "            hits, cells, particles, truth = load_event(os.path.join(path_to_data, name))\n",
    "\n",
    "            # Generate new vector of particle id\n",
    "            particle_ids = truth.particle_id.values\n",
    "            particle2track = {}\n",
    "            for pid in np.unique(particle_ids):\n",
    "                particle2track[pid] = track_id\n",
    "                track_id += 1\n",
    "            hits['particle_id'] = [particle2track[pid] for pid in particle_ids]\n",
    "\n",
    "            # Collect hits\n",
    "            events.append(hits)\n",
    "        except:\n",
    "            print(\"Error with\", name)\n",
    "            continue\n",
    "            \n",
    "    # Put all hits into one sample with unique track ids\n",
    "    data = pd.concat(events, axis=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "24fa34add5bc14aa646b4312d381983cbc58750f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "start_event_id = 1000\n",
    "n_train_samples = 5\n",
    "train_event_names = [\"event0000{:05d}\".format(i) for i in range(start_event_id, start_event_id+n_train_samples)]\n",
    "train_data = get_training_sample(path_to_train, train_event_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(583142, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9606479b7073efdaa2788331cf530dc2c0fadb91"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2795efcf5f34cfd4eaaf4d169f3824c67446e6d1"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0eeacfd9a2210b0fedb886ad1236e4e30f8a77f"
   },
   "source": [
    "Then, train the classifier using this sample. Notice that data preprocessing is included into the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "77aa9594ab7fafa8efc3ee38e367c347a65d25be"
   },
   "outputs": [],
   "source": [
    "class Clusterer(object):\n",
    "    \n",
    "    def __init__(self,rz_scales=[0.65, 0.965, 1.528], eps=0.0035):\n",
    "        self.classifier = None\n",
    "        self.rz_scales = rz_scales\n",
    "    \n",
    "    def _init(self,dfh):\n",
    "        dfh['r'] = np.sqrt(dfh['x'].values**2+dfh['y'].values**2+dfh['z'].values**2)\n",
    "        dfh['rt'] = np.sqrt(dfh['x'].values**2+dfh['y'].values**2)\n",
    "        dfh['a0'] = np.arctan2(dfh['y'].values,dfh['x'].values)\n",
    "        dfh['z1'] = dfh['z'].values/dfh['rt'].values\n",
    "        dfh['x2'] = 1/dfh['z1'].values\n",
    "        dz0 = -0.00070\n",
    "        stepdz = 0.00001\n",
    "        stepeps = 0.000005\n",
    "        mm = 1\n",
    "        for ii in tqdm(range(100)):\n",
    "            mm = mm*(-1)\n",
    "            dz = mm*(dz0+ii*stepdz)\n",
    "            dfh['a1'] = dfh['a0'].values+dz*dfh['z'].values*np.sign(dfh['z'].values)\n",
    "            dfh['sina1'] = np.sin(dfh['a1'].values)\n",
    "            dfh['cosa1'] = np.cos(dfh['a1'].values)\n",
    "            dfh['x1'] = dfh['a1'].values/dfh['z1'].values\n",
    "            ss = StandardScaler()\n",
    "            dfs = ss.fit_transform(dfh[['sina1','cosa1','z1','x1','x2']].values)\n",
    "            cx = np.array([1, 1, 0.75, 0.5, 0.5])\n",
    "            dfs = np.multiply(dfs, cx)\n",
    "\n",
    "            clusters=DBSCAN(eps=self.epsilon+ii*stepeps,min_samples=1,metric='euclidean', n_jobs=1).fit(dfs).labels_            \n",
    "            if ii==0:\n",
    "                dfh['s1'] = clusters\n",
    "                dfh['N1'] = dfh.groupby('s1')['s1'].transform('count')\n",
    "            else:\n",
    "                dfh['s2'] = clusters\n",
    "                dfh['N2'] = dfh.groupby('s2')['s2'].transform('count')\n",
    "                maxs1 = dfh['s1'].max()\n",
    "                cond = np.where((dfh['N2'].values>dfh['N1'].values) & (dfh['N2'].values<20))\n",
    "                s1 = dfh['s1'].values\n",
    "                s1[cond] = dfh['s2'].values[cond]+maxs1\n",
    "                dfh['s1'] = s1\n",
    "                dfh['s1'] = dfh['s1'].astype('int64')\n",
    "                dfh['N1'] = dfh.groupby('s1')['s1'].transform('count')\n",
    "        return dfh['s1'].values   \n",
    "    \n",
    "    def _preprocess(self, hits):\n",
    "        x = hits.x.values\n",
    "        y = hits.y.values\n",
    "        z = hits.z.values\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        hits['x2'] = x/r\n",
    "        hits['y2'] = y/r\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        hits['z2'] = z/r\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "        for i, rz_scale in enumerate(self.rz_scales):\n",
    "            X[:,i] = X[:,i] * rz_scale\n",
    "       \n",
    "        return X\n",
    "    \n",
    "    def fit(self, hits):\n",
    "        \n",
    "        X = self._preprocess(hits)\n",
    "        y = hits.particle_id.values\n",
    "        \n",
    "        self.classifier = GaussianMixture(n_components=100, verbose=1)\n",
    "        self.classifier.fit(X, y)\n",
    "    \n",
    "    \n",
    "    def predict(self, hits):\n",
    "        \n",
    "        X = self._preprocess(hits)\n",
    "        labels = self.classifier.predict(X)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fded561ee3b987449bebf2b8f86d264ed2c0fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n"
     ]
    }
   ],
   "source": [
    "model = Clusterer()\n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee321a07c998159f5593e258a12041447cf6741f"
   },
   "source": [
    "### Test\n",
    "\n",
    "Use the trained classifier to predict labels of hits in a new event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95e4ba95eed9021bef72e9e99b495bed3025227f"
   },
   "outputs": [],
   "source": [
    "path_to_event = os.path.join(path_to_train, \"event0000{:05d}\".format(start_event_id + n_train_samples + 1))\n",
    "hits, cells, particles, truth = load_event(path_to_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c00780319688bd08aee457f57f040a69e64e37cf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Warning: it takes about 30s per one event\n",
    "labels = model.predict(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58bc645bd7d458880ec545c7195e6ea4630f4897"
   },
   "source": [
    "# Score\n",
    "\n",
    "Calculate quality of the track pattern recognition for one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb8f59cda1c508b38677a0c6292f6d2daed868bf"
   },
   "outputs": [],
   "source": [
    "submission = create_one_event_submission(0, hits, labels)\n",
    "score = score_event(truth, submission)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cdf9a349c37d400be6be060ec52762de49eaf71"
   },
   "source": [
    "# Recognize tracks in all events of a dataset\n",
    "\n",
    "In this example, the dataset is the whole training set. This may take a very long time. To run on only a subset, use\n",
    "\n",
    "     load_dataset(path_to_train, skip=1000, nevents=5)\n",
    "\n",
    "It will skip the first 1000 events, and select the next 5 ones.\n",
    "\n",
    "**Warning:** it takes about 30s per one event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "96451a3784850487927e5d4654fc97801277f2cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1010: 0.065\n",
      "Score for event 1011: 0.068\n",
      "Score for event 1012: 0.065\n",
      "Score for event 1013: 0.078\n",
      "Score for event 1014: 0.086\n",
      "Mean score: 0.073\n"
     ]
    }
   ],
   "source": [
    "dataset_submissions = []\n",
    "dataset_scores = []\n",
    "\n",
    "for event_id, hits, cells, particles, truth in load_dataset(path_to_train, skip=10, nevents=5):\n",
    "        \n",
    "    # Track pattern recognition\n",
    "    labels = model.predict(hits)\n",
    "        \n",
    "    # Prepare submission for an event\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "    dataset_submissions.append(one_submission)\n",
    "    \n",
    "    # Score for the event\n",
    "    score = score_event(truth, one_submission)\n",
    "    dataset_scores.append(score)\n",
    "    \n",
    "    print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "    \n",
    "print('Mean score: %.3f' % (np.mean(dataset_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a262ea84036ed15ebab34d87faf8a7dc3826f74f"
   },
   "source": [
    "# Submission\n",
    "\n",
    "Create a submission file.\n",
    "\n",
    "**Warning:** it takes about 30s per one event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "507033bb38f02bf16fa8f04a83cf191d5a286c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event ID:  0\n",
      "Event ID:  1\n",
      "Event ID:  2\n",
      "Event ID:  3\n",
      "Event ID:  4\n",
      "Event ID:  5\n",
      "Event ID:  6\n",
      "Event ID:  7\n",
      "Event ID:  8\n",
      "Event ID:  9\n",
      "Event ID:  10\n",
      "Event ID:  11\n",
      "Event ID:  12\n",
      "Event ID:  13\n",
      "Event ID:  14\n",
      "Event ID:  15\n",
      "Event ID:  16\n",
      "Event ID:  17\n",
      "Event ID:  18\n",
      "Event ID:  19\n",
      "Event ID:  20\n",
      "Event ID:  21\n",
      "Event ID:  22\n",
      "Event ID:  23\n",
      "Event ID:  24\n",
      "Event ID:  25\n",
      "Event ID:  26\n",
      "Event ID:  27\n",
      "Event ID:  28\n",
      "Event ID:  29\n",
      "Event ID:  30\n",
      "Event ID:  31\n",
      "Event ID:  32\n",
      "Event ID:  33\n",
      "Event ID:  34\n",
      "Event ID:  35\n",
      "Event ID:  36\n",
      "Event ID:  37\n",
      "Event ID:  38\n",
      "Event ID:  39\n",
      "Event ID:  40\n",
      "Event ID:  41\n",
      "Event ID:  42\n",
      "Event ID:  43\n",
      "Event ID:  44\n",
      "Event ID:  45\n",
      "Event ID:  46\n",
      "Event ID:  47\n",
      "Event ID:  48\n",
      "Event ID:  49\n"
     ]
    }
   ],
   "source": [
    "path_to_test = \"data/test\"\n",
    "test_dataset_submissions = []\n",
    "\n",
    "create_submission = True\n",
    "\n",
    "if create_submission:\n",
    "    for event_id, hits, cells in load_dataset(path_to_test, parts=['hits', 'cells']):\n",
    "\n",
    "        # Track pattern recognition\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        test_dataset_submissions.append(one_submission)\n",
    "        \n",
    "        print('Event ID: ', event_id)\n",
    "\n",
    "    # Create submission file\n",
    "    submission = pd.concat(test_dataset_submissions, axis=0)\n",
    "    submission.to_csv('20180626_submission_3.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21e770eb43774395f4983ace8093508c1dd3f69a",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
